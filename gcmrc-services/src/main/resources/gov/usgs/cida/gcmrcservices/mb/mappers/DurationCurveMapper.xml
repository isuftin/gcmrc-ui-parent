<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mapper
  PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
    "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
 
<mapper namespace="gov.usgs.cida.gcmrcservices.mb.mappers.DurationCurveMapper">
	
	<resultMap id="durationCurveResult" type="DurationCurvePoint">
		<result property="binNumber" column="BIN_NUMBER"/>
		<result property="cumulativeBinPerc" column="CUMULATIVE_BIN_PERC"/>
		<result property="binValue" column="BIN_VALUE"/>
		<result property="inBinMinutes" column="IN_BIN_MINUTES"/>
		<result property="cumulativeInBinMinutes" column="CUMULATIVE_IN_BIN_MINUTES"/>
		<result property="lowBound" column="LOW_BOUND"/>
		<result property="highBound" column="HIGH_BOUND"/>
	</resultMap>
        
	<resultMap id="durationCurveGapMinutesResult" type="DurationCurveCumulativeGap">
		<result property="gapMinutesPercent" column="GAP_MINUTES_PERCENT"/>
		<result property="gapMinutes" column="GAP_MINUTES"/>
	</resultMap>

	<resultMap id="durationCurveConsecutiveGapResult" type="DurationCurveConsecutiveGap">
		<result property="gapTime" column="GAP_TIME"/>
		<result property="gapUnit" column="GAP_UNIT"/>
	</resultMap>
        
        <sql id="withConstants">
            const (one_hour_of_minutes, one_day_of_minutes, data_gap_marker, max_gap_minutes, max_interpolation_days, min_log_bin_value, input_start_date, input_end_date, site_id, group_id) as
				(
				   <!-- CONSTANTS

					 INTERPOLATION SETTINGS **
					In general, it is assumed that real data exists between individual measurements and the value of
					that real data can be determined by interpolation.  However, it wouldn't make sense to interpolate
					b/t two values *very* far apart in time - that limit is set by MAX_INTERPOLATION_DAYS.

					Sometime there is a known time period of invalid data.  This is recorded as a time period bounded
					by the DATA_GAP_MARKER in the FINAL_VALUE column.  The DB ensures that no values occur in the timeseries
					b/t DATA_GAP_MARKER's.  The DATA_GAP_MARKER is always -999.

					Interpolation can still be done through invalid time periods if the invalid time period is less than
					MAX_GAP_MINUTES.

					ASSUMPTION:  THE DB CONTAINS NO DATA WITH -999 VALUES THAT ARE NOT CONSECUTIVE.

					-->
					 select
					   60 one_hour_of_minutes,
					   60*24 one_day_of_minutes,
					   -999 data_gap_marker, <!-- Known value in the db used to mark the start and end of a timespan for which we do not have data. -->
					   60 max_gap_minutes, <!-- Max single invalid timespan that interpolation is allowed through. -->
					   <!-- Max days for which interpolation is allowed between two measured values. -->
					   60 max_interpolation_days,
					   0.1 min_log_bin_value, <!-- Min value to use for bins.  Log bins don't allow zeros and low values are basically noise -->
					   to_timestamp(#{starttime, jdbcType=CHAR}, 'YYYY-MM-DD"T"HH24:MI:SS') input_start_date, <!-- Makes for easier debugging when you can just substitute the date in one place -->
					   to_timestamp(#{endtime, jdbcType=CHAR}, 'YYYY-MM-DD"T"HH24:MI:SS') input_end_date,
					 (select 
						site_id
						from 
							(select 
								site_id,
								coalesce(nwis_site_no, short_name) site_name
								from 
								site_star) a
						where 
							site_name = #{sitename, jdbcType=CHAR}) site_id,
						#{groupid, jdbcType=NUMERIC}::numeric group_id)
        </sql>
        <sql id="withRawMtiData">
            <include refid="withConstants"/>,
            raw_mti_data (group_id, start_value, end_value, start_time, end_time, gap_minutes) as (

					 <!-- Create a coarsely filtered set of data where each row is a Measured Time Interval (MTI).
					   The rows returned from this query will span slightly beyond the fromTime and toTime (MAX_INTERPOLATION_DAYS
					   beyond each limit).  Invalid gaps (marked with DATA_GAP_MARKERs) will be reomved and interpolation
					   over gaps will be done.

					   A MTI is a period of known time and measured values that occurs between two individual measurements.
					   Thus, it has a starting value, an ending value, a starting time and an ending time.  As an example,
					   in the diagram below, the vertical lines represent rows in the db and the space between are the
					   rows created in this query:

							|   MTI1   |   MTI2   |   MTI3 (has no end time or end value)   
						   DB1        DB2        DB3

					   The db countains measured time-ordered results DB1, DB2 and DB3, each with a time and value.
					   This result of the example above would be:
						MTI1: Start_Time and Start_Value from the time and value of DB1, End_Time
						 and End_Value from the time and value of DB2.
						MTI2:  Similar to Result1, just shifted.
						MTI3:  Will have a null End_Time and End_Value
					 -->

					 select

					   <!-- Bridge invalid data gaps w/o consideration for the gap duration or the time b/t good values.
					   We've already filtered out the blocks that begin AND end w/ -999, so we are left
					   with consecutive rows that look like this:
					   _____________________
					   Start Val | End Val
					   123       | -999
					   -999      | 234
					   _____________________
					   We bridge these values into the first row and mark as TRUE in the IS_BRIDGE column.
					   Start Val | End Val | IS_BRIDGED
					   123       | 234     | 1
					   -999      | 234     | 0
					   _____________________
					   The 2nd row is unmodified here (its removed in the outer select).
					   -->
					   group_id,
					   start_value,
					   case when end_value = (select data_gap_marker from const) then lead(end_value) over (order by start_time) else end_value end as end_value,
					   start_time,
					   case when end_value = (select data_gap_marker from const) then lead(end_time) over (order by start_time) else end_time end as end_time,
					   gap_minutes <!-- How many minutes in the gap (if any) in the following bin -->
					 from (
					   select * from ( <!-- WHERE removes invalid data rows (start and end values = DATA_GAP_MARKER -->
						 select
						   <!-- Record data gap durations into the row before the gap (gaps removed in outer query) -->
						   group_id,
						   start_value,
						   end_value,
						   start_time, <!-- Can we be sure there are no duplicate date entries for a site? Yes. -->
						   end_time,
						   <!-- If the row after this one is a data gap, record the duration of the gap in this row -->
						   case
							 when lead(start_value) over (order by start_time) = (select data_gap_marker from const) and lead(end_value) over (order by start_time) = (select data_gap_marker from const)
							   then (extract(epoch from ((lead(end_time) over (order by start_time) - start_time)))/(select one_hour_of_minutes from const))::numeric
							 else 0
						   end as gap_minutes
						 from (
						   select
							 <!-- Selects the measured values at the site, creating a start/end time/value for each row
							 from the db rows, which just contain a single measurement at a single time.  thus
							 one resulting row contains the start and end times and values of the two
							 db rows that bound it.

							 the coarse time filter keeps only rows in the specified range + the max interpolation time period. -->
							 group_id,
							 final_value as start_value,
							 lead(final_value) over (order by measurement_date) as end_value,
							 measurement_date as start_time, <!-- Can we be sure there are no duplicate date entries for a site? Yes. -->
							 lead(measurement_date) over (order by measurement_date) as end_time
						   from time_series_star
						   where 
							<!-- Only this site and selected parameter -->
							(site_id, group_id) in (select site_id, group_id from const)
							 <!-- Filter the time window to the specified range + the max interpolation time period -->
							 and measurement_date > ((select input_start_date from const) - cast((select max_interpolation_days from const) || 'D' as interval))
							 and measurement_date &lt; ((select input_end_date from const) + cast((select max_interpolation_days from const) || 'D' as interval))
						 ) ab
					   ) abc where
						 not (start_value = (select data_gap_marker from const) and end_value = (select data_gap_marker from const)) 
					 ) d
				   )
        </sql>
        <sql id="withRawGapData">
            <include refid="withConstants"/>,
            raw_gap_data (site_id, group_id, start_time, gap_status) AS (
                select
                /* level 1: mark appropriate rows as "gap starts" or "gap stops" and limits rows to those */
                /*          overlapping the INPUT_START_DATE - INPUT_END_DATE range (for site_id/group_id)*/
                site_id,
                group_id,
                greatest(measurement_date, (select input_start_date from const)) start_time,
                case when coalesce(lag(final_value) over (order by measurement_date), 0) != (select data_gap_marker from const) and final_value = (select data_gap_marker from const) then 'gap starts'
                     when lag(final_value) over (order by measurement_date) = (select data_gap_marker from const) and final_value != (select data_gap_marker from const) then 'gap ends'
                end gap_status
                from
                  time_series_star
                where     
                  (site_id, group_id) in (select site_id, group_id from const) and
                   measurement_date between 
                      coalesce(
                          (select 
                              /* determine most recent measurement_date for site/group preceding INPUT_START_DATE */
                              max(measurement_date)
                           from 
                              time_series_star
                           where 
                              (site_id, group_id) in (select site_id, group_id from const) and 
                               measurement_date &lt;= (select input_start_date from const)
                          ),
                          (select input_start_date from const))
                                   and /* second half of "between..." */
                          (select input_end_date from const)
            )
        </sql>
        <sql id="withGapDuration">
            <include refid="withRawGapData"/>,
            gap_duration (site_id, group_id, gap_start_time, gap_end_time, gap_status)
            as 
            (select 
                /* level 2: using only "gap starts" and "gap ends" rows, calculate gap_end_time */
                site_id,
                group_id,
                start_time gap_start_time,
                lead(start_time, 1, (select input_end_date from const)) over (order by start_time) gap_end_time,
                gap_status
              from
                raw_gap_data
                where 
                   gap_status in ('gap starts', 'gap ends')
             )
        </sql>
        <sql id="withCleanMtiData">
            <include refid="withRawMtiData"/>,
            clean_mti_data (group_id, start_value, end_value, min_value, max_value, start_time, end_time, duration_minutes, gap_minutes, is_start_interpolated, is_end_interpolated) as (
					 <!-- Further clean Measured Time Intervals (MTI) to remove interpolation ranges that are either too
					   long or contain gaps that are too long.  Some other useful columns are added.

					   See RAW_MTI_DATA for details on what an MTI is.

					   The final MTI values will be trimmed to only contain MTI that overlap the specified time range.
					   In addition, if the first or last MTIs only partially overlap the time range,
					   they will be trimmed to exactly match the time range and their start or end
					   values interpolated to the new start or end time.
					 -->
					 select
					   group_id,
					   start_value, end_value, 
					   least(start_value, end_value) as min_value,
					   greatest(start_value, end_value) as max_value,
					   start_time,
					   end_time,
					   (extract(epoch from ((end_time - start_time)))/(select one_hour_of_minutes from const))::numeric as duration_minutes,
					   gap_minutes, is_start_interpolated, is_end_interpolated <!-- These are all for debugging -->
					 from (
					   select
					   group_id,
						 <!--
						 Interpolation of the start value (applied to the single row which would have a start time b/f the user spec'ed start time.
						 V1 = START_VALUE, V2 = END_VALUE, T1 = START_TIME, T2 = END_TIME, Ts = User Spec'ed fromTime.
						 Vs = Interpolated value at Ts:
						 Vs = V1 + (Tx - T1) * ((V2 - V1) / (T2 - T1))
						 -->
						case
						when start_time &lt; (select input_start_date from const) 
						then start_value + ((extract(epoch from ((( select input_start_date from const) - start_time)))/(select one_hour_of_minutes from const))::numeric) 
							* ((end_value - start_value)/((extract(epoch from ((end_time - start_time)))/(select one_hour_of_minutes from const))::numeric))
						else start_value
						end as start_value,
						 <!--
						 Interpolation of the end value (applied to the single row which would have a end time after the user spec'ed end time.
						 Same equation as above, but Ts is the user spec'ed toTime.
						 -->
						 case
						when end_time > (select input_end_date from const) then 
						start_value + ((extract(epoch from (((select input_end_date from const) - start_time)))/(select one_hour_of_minutes from const))::numeric) * ((end_value - start_value)/((extract(epoch from ((end_time - start_time)))/(select one_hour_of_minutes from const))::numeric))
						else end_value
						end as end_value,
						 <!-- Adjust start and end times to be the spec'ed from/to times if they fall outside the spec'ed range -->
						 case when start_time &lt; (select input_start_date from const) then (select input_start_date from const) else start_time end as start_time,
						 case when end_time > (select input_end_date from const) then (select input_end_date from const) else end_time end as end_time,
						 gap_minutes,
						 case when start_time &lt; (select input_start_date from const) then 1 else 0 end as is_start_interpolated,
						 case when end_time > (select input_end_date from const) then 1 else 0 end as is_end_interpolated
					   from raw_mti_data
					   where
						 start_value != (select data_gap_marker from const) <!-- Already filtered out end value and double values - this is the only case left -->
						 and end_time > (select input_start_date from const)
						 and start_time &lt; (select input_end_date from const)
						 and not gap_minutes > (select max_gap_minutes from const)
						 and not extract(day from (end_time - start_time)) > (select max_interpolation_days from const)
					   order by start_time <!-- Used for debugging -->
					 ) e order by start_time <!-- Used for debugging -->
				   )
        </sql>
        <sql id="withData">
            <include refid="withCleanMtiData"/>,
            const_data (group_id, gap_minutes, first_measure_date, last_measure_date, overall_duration_minutes, user_duration_minutes, overall_min_value, overall_max_value, overall_value_range, measurement_count) as
					(
					 select
					   group_id,
					   sum(gap_minutes) gap_minutes,
					   min(start_time) as first_measure_date,
					   max(end_time) as last_measure_date,
					   sum(duration_minutes) as overall_duration_minutes,
					   (extract(epoch from ((select input_end_date from const) - (select input_start_date from const)))/(select one_hour_of_minutes from const))::numeric as user_duration_minutes,
					   min(min_value) as overall_min_value,
					   max(max_value) as overall_max_value,
					   max(max_value) - min(min_value) as overall_value_range,
					   count(*) as measurement_count
					 from clean_mti_data
					group by group_id
            )
        </sql> 
        <sql id="withGapData">
        <include refid="withGapDuration"/>, 
        const_gap_data (site_id, 
            group_id,
            gap_start_time,
            gap_end_time,
            gap_minutes,
            rn) as
           (select 
               /* level 3: limit the rows to the ones marked "gap starts" */
               /*          so each row represents a gap in measurements   */
               site_id,
               group_id,
               gap_start_time,
               gap_end_time,
               ((extract(DAY from (gap_end_time - gap_start_time)) * 24 * 60 * 60 +
               extract(HOUR from (gap_end_time - gap_start_time)) * 60 * 60 +
               extract(MINUTE from (gap_end_time - gap_start_time)) * 60 +
               extract(SECOND from (gap_end_time - gap_start_time)))/60)::numeric gap_minutes,
               row_number() over (order by 
                  extract(DAY from (gap_end_time - gap_start_time)) * 24 * 60 * 60 +
                  extract(HOUR from (gap_end_time - gap_start_time)) * 60 * 60 +
                  extract(MINUTE from (gap_end_time - gap_start_time)) * 60 +
                  extract(SECOND from (gap_end_time - gap_start_time)) desc, gap_start_time) rn
            from
              gap_duration
              where 
                gap_status = 'gap starts'
           )
        </sql>
        <sql id="withIntBins">
            int_bins (bin_number) as
				   (
					 select generate_series(1,#{bincount, jdbcType=NUMERIC}::numeric) bin_number
				   )
        </sql>
        <sql id="withLinBins">
            lin_bins (bin_number, low_bound, high_bound) as (
				   <!-- Linear spaced bins -->
					 select
					   bin_number,
					   (select ((overall_value_range / #{bincount, jdbcType=NUMERIC}::numeric) * (bin_number - 1)) + overall_min_value from const_data) as low_bound,
					   case
					   when bin_number = #{bincount, jdbcType=NUMERIC}::numeric then (select overall_max_value from const_data) <!-- For the last bin, use the actual high value to prevent rounding errors -->
					   else (select ((overall_value_range / #{bincount, jdbcType=NUMERIC}::numeric) * bin_number) + overall_min_value from const_data) 
					   end as high_bound
					 from int_bins
					 order by bin_number
				   )
        </sql> 
        <sql id="withLogBins">
            log_bins (bin_number, low_bound, high_bound) as (
				   <!-- Log spaced bins
					 log bins may include a zero bin that goes from the lowest measured value to the min_log_bin value, since we
					 can't do log calcs on zero.  This only occurs when the lowest value is below the MIN_LOG_BIN_VALUE. -->
					select 
						0 bin_number, (select overall_min_value from const_data) low_bound, (select min_log_bin_value from const) high_bound
					where (select overall_min_value from const_data) &lt; (select min_log_bin_value from const)
					union all
					select
						bin_number,
						case when bin_number = 1 then (select greatest(overall_min_value, (select min_log_bin_value from const)) from const_data) else power(10, log_low_bound) end as low_bound,
						case when bin_number = #{bincount, jdbcType=NUMERIC}::numeric then (select overall_max_value from const_data) else power(10, log_high_bound) end as high_bound
					from (
						select
							bin_number,
							(select (((log(10, overall_max_value::numeric) - log(10, greatest(overall_min_value, (select min_log_bin_value from const))::numeric)) / #{bincount, jdbcType=NUMERIC}::numeric) * (bin_number - 1)) + log(10, greatest(overall_min_value, (select min_log_bin_value from const))::numeric) from const_data) as log_low_bound,
							(select (((log(10, overall_max_value::numeric) - log(10, greatest(overall_min_value, (select min_log_bin_value from const))::numeric)) / #{bincount, jdbcType=NUMERIC}::numeric) * bin_number) + log(10, greatest(overall_min_value, (select min_log_bin_value from const))::numeric) from const_data) as log_high_bound
						from int_bins
						order by bin_number
					) f
				)
        </sql>
	<select id="getDurationCurve" parameterType="map" resultMap="durationCurveResult">	
            with
            <include refid="withData"/>,
            <include refid="withIntBins"/>,
            <include refid="withLinBins"/>,
            <include refid="withLogBins"/> 
				   select 
					 bin_number,
					 sum(in_bin_minutes) over (order by bin_number desc rows between unbounded preceding and current row) /
					   (select overall_duration_minutes from const_data) * 100 as cumulative_bin_perc,
					 case
					   when bin_number = 1 then low_bound
					   when bin_number = #{bincount, jdbcType=NUMERIC}::numeric then high_bound
					   else (low_bound + high_bound) / 2
					 end as bin_value,
					 in_bin_minutes,
					 sum(in_bin_minutes) over (order by bin_number desc rows between unbounded preceding and current row) as cumulative_in_bin_minutes,
					 low_bound,
					 high_bound
				   from (
					 select
					   sum(in_bin_minutes) as in_bin_minutes,
					   bin_number,
					   max(low_bound) as low_bound,
					   max(high_bound) as high_bound
					 from (

				   <!-- POSSIBLE PERFORMANCE OPTIMIZATION:
					 Rather than join all possible data-bin combinations and then use CASE to
					 select the correct calculation, it might be faster to individually join the
					 different types of data-bin combos and UNION the results togetheer.  The advantage
					 would be that the CASE statements would not be needed because the data-bin
					 relation would be known.  The down side would that that multiple passes through
					 the dataset would be required, so its not clear that it would be faster.
				   -->
					   select
						 case
						   <!--
							All these CASE statements deal w/ handling 4 possible cases for a Measured
							Data Chunk (MTI).  A MTI block of known time and values and it occurs between
							two individual measurements.  Thus, it has a starting value, an ending value
							and a duration.  It doesn't matter for the calculation if the values are
							increasing from initial measure to ending measure, so instead of working with
							the start and end value, most of the comparisons are done using min and max
							values of an MTI.

							The bins define a high and low bound, and there are four cases of how an
							MTI could line up with an individual bin:
							|      |      |      |
							|    *=|======|===*  | Case 1:  The MTI spans beyond both the upper and lower bin bounds
							|   *==|==*   |      | Case 2:  The MTI spans beyond the lower bound (but not the upper)
							|      |   *==|=*    | Case 3:  The MTI spans beyond the upper bound (but not the lower)
							|      | *==* |      | Case 4:  The MTI is completely within the upper and lower bounds of the bin
							|      |      |      |
							======================
							|  B1L |======| B1U  | The bounds of a single bin (B1) with its upper and lower bounds
							-->

						   <!-- Case 1: Time in bin is related to the ratio of bin_range to data_range.  Time in bin = (bin_range / data_range) X MTI duration -->
						   when data.min_value &lt; bin.low_bound and data.max_value > bin.high_bound then ((bin.high_bound - bin.low_bound) / (data.max_value - data.min_value)) * data.duration_minutes

						   <!-- Case 2: Time in bin is related to the ratio of the data range withing the bin to the entire data_range. -->
						   when data.min_value &lt; bin.low_bound and data.max_value &lt;= bin.high_bound then ((data.max_value - bin.low_bound) / (data.max_value - data.min_value)) * data.duration_minutes

						   <!-- Case 3: similar to case 3, just calc from the upper bin bound -->
						   when data.min_value &lt; bin.high_bound and data.max_value > bin.high_bound then ((bin.high_bound - data.min_value) / (data.max_value - data.min_value)) * data.duration_minutes

						   <!-- Case 4:  The entire duration of the MTI is spend in the bin -->
						   else data.duration_minutes
						 end as in_bin_minutes,
						 min_value, max_value, low_bound, high_bound, duration_minutes,
						 start_value, end_value, start_time, end_time, bin_number, gap_minutes
					   from ${binType} bin inner join clean_mti_data data on
						 (data.min_value &lt; bin.high_bound and data.max_value > bin.low_bound) <!-- Std case where data values do not fall on bin boundaries -->
						 or (data.min_value = data.max_value and data.min_value = bin.low_bound) <!-- Single value case that falls on a bin bondaries (goes w/ the bin above that value) -->
						 or (data.min_value = bin.high_bound and bin.bin_number = #{bincount, jdbcType=NUMERIC}::numeric) <!-- Single values falling on a bin boundary go in the bin above it, however, a single value that falls on the top bin's upper bound goes in that bin. -->
					   order by bin.bin_number, start_time
					 ) g
					 group by bin_number
					 order by bin_number <!-- debug -->
				   ) h
				   order by bin_number
	</select>	
        <select id="getDurationCurveCumulativeGap" parameterType="map" resultMap="durationCurveGapMinutesResult">	
            with
            <include refid="withGapData"/>
            select round((gaps.gap_minutes/duration.duration_minutes*100)::numeric,1) as gap_minutes_percent, gaps.gap_minutes from
            (select
            (extract(DAY from (max(start_time) - min(start_time))) * 24 * 60 * 60 +
             extract(HOUR from (max(start_time) - min(start_time))) * 60 * 60 +
             extract(MINUTE from (max(start_time) - min(start_time))) * 60 +
             extract(SECOND from (max(start_time) - min(start_time))))/60 duration_minutes
              from
                 raw_gap_data) duration,
                 (select sum(gap_minutes) gap_minutes from const_gap_data) gaps
        </select>
        <select id="getDurationCurveConsecutiveGap" parameterType="map" resultMap="durationCurveConsecutiveGapResult">
          with
            <include refid="withGapData"/>
            select 
                case 
                when round((gap_minutes/60)::numeric,2) &lt; 24 then to_char(round((gap_minutes/60)::numeric,2),'999D99') 
                when round((gap_minutes/60)::numeric,2) >= 24 then to_char(round((gap_minutes/60/24)::numeric,2),'999D99')
                end gap_time,
                case 
                when round((gap_minutes/60)::numeric,2) &lt; 24 then 'hours'
                when round((gap_minutes/60)::numeric,2) >=24 then 'days'
                end gap_unit
           from const_gap_data
           where rn = 1
        </select>
</mapper>